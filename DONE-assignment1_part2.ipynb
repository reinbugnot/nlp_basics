{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abe9e48010928487f05fb692a8fc93d1",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1 Part 2: N-gram Language Models (Cont.) (30 pts)\n",
    "\n",
    "In this assignment, we're going to train an n-gram language model that is able to \"imitate\" William Shakespeare's writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73e5265758b71680e006b1db39c741bf",
     "grade": false,
     "grade_id": "cell-125dc5645c8de34a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy and paste the functions you wrote in Part 1 here and import any libraries necessary\n",
    "# We have tried a more elegant solution by using\n",
    "# from ipynb.fs.defs.assignment1_part1 import load_data, build_vocab, build_ngrams\n",
    "# but it doesn't work with the autograder...\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize, TweetTokenizer, word_tokenize\n",
    "    import re\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    with open('assets/gutenberg/THE_SONNETS.txt', 'r') as file:\n",
    "        sentences = file.read()[3:]\n",
    "        \n",
    "    # Fix decoding error\n",
    "    sentences = sentences.encode('cp1252') \n",
    "    sentences = sentences.decode('utf-8')\n",
    "    \n",
    "    sentences = [i.strip(' ').lower() for i in sentences.split('\\n') if len(i.strip(' ').split(' ')) > 3]\n",
    "    sentences = [nltk.word_tokenize(i) for i in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "stu_ans = load_data()\n",
    "\n",
    "def unique(str_list): # custom helper function for a cleaner build_vocab function\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for x in str_list:\n",
    "        if x not in output:\n",
    "            output.append(x)\n",
    "    output.append('<s>')\n",
    "    output.append('</s>')\n",
    "    \n",
    "    return output\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = []\n",
    "    sentences_flat = [item for sublist in sentences for item in sublist]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    vocab = unique(sentences_flat)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "from nltk import ngrams\n",
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for sample in sentences:\n",
    "        \n",
    "        tmp = sample.copy()\n",
    "        \n",
    "        for i in range(n-1):\n",
    "            tmp.insert(0, '<s>')\n",
    "            tmp.append('</s>')\n",
    "            \n",
    "        all_ngrams.append([i for i in ngrams(tmp, n)])\n",
    "\n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e6de29b5777fafd982319bf679e496f",
     "grade": false,
     "grade_id": "cell-e4ca1cd2de4ef7da",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4: Guess the next token (20 pts)\n",
    "\n",
    "With the help of the three functions you wrote in Part 1, let's first answer the following question as a review on $n$-grams.\n",
    "\n",
    "Assume we are now working with bi-grams. What is the most likely token that comes after the sequence `<s> <s> <s>`, and how likely? Remember that a bi-gram language model is essentially a first-order Markov Chain. So, what determines the next state in a first-order Markov Chain? \n",
    "\n",
    "**Complete the function below to return a `tuple`, where `tuple[0]` is a `str` representing the mostly likely token and `tuple[1]` is a `float` representing its (conditional) probability of being the next token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25120/3759784470.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprev_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<s>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbigram_matches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprev_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbigram_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_matches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnext_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = load_data()\n",
    "bigrams = build_ngrams(2, sentences)\n",
    "prev_token = '<s>'\n",
    "bigram_matches = [[j for j in i if j[0] == prev_token][0] for i in bigrams]\n",
    "bigram_freq = dict(collections.Counter(bigram_matches))\n",
    "next_token = max(bigram_freq, key=bigram_freq.get)[1]\n",
    "prob = bigram_freq[max(bigram_freq, key=bigram_freq.get)] / sum(bigram_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d19295ea8f314fb6b49f963689e9609",
     "grade": false,
     "grade_id": "cell-f0590ac9b51f337e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import collections # easier frequency count\n",
    "\n",
    "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
    "    \"\"\"\n",
    "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
    "    \"\"\"\n",
    "    \n",
    "    # load dataset\n",
    "    sentences = load_data()\n",
    "    \n",
    "    # build the bigrams\n",
    "    bigrams = build_ngrams(2, sentences)\n",
    "    \n",
    "    # get the prev_token (Wn_1) which should be the last token in start_tokens\n",
    "    prev_token = start_tokens[-1]\n",
    "    \n",
    "    # get all the bigrams that starts with prev_token (using list comprehension to parse through 2d list of tuples)\n",
    "    bigram_matches = [[j for j in i if j[0] == prev_token][0] for i in bigrams]\n",
    "    \n",
    "    # use collections.Counter to count the occurences of each unique bigram tuple\n",
    "    bigram_freq = dict(collections.Counter(bigram_matches))\n",
    "    \n",
    "    # the most probably next_token is the one that has the highest probability (which also means it has the highest frequency)\n",
    "    next_token = max(bigram_freq, key=bigram_freq.get)[1]\n",
    "    \n",
    "    # calculate the probability of the most occuring bigram by dividing the frequency value of the bigram \n",
    "    # to the total population of all bigrams that begins with prev_token\n",
    "    # i.e. P(W_n | W_n-1) = P(W_n-1, W_n) / P(W_n-1) where W_n-1 = prev_token (or '<s>') and W_n is the word we're looking for.\n",
    "    prob = bigram_freq[max(bigram_freq, key=bigram_freq.get)] / sum(bigram_freq.values())\n",
    "    \n",
    "    return next_token, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da8b5be8d6a7ec8ba658634873e4edf4",
     "grade": true,
     "grade_id": "cell-6a94a4513c67c4d5",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram_next_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25120/1373281488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Autograder tests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstu_ans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_next_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<s>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstu_ans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Q4: Your function should return a tuple. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram_next_token' is not defined"
     ]
    }
   ],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = bigram_next_token(start_tokens=(\"<s>\", ) * 3)\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q4: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q4: Your tuple should have two elements. \"\n",
    "assert isinstance(stu_ans[0], str), \"Q4: tuple[0] should be a str. \"\n",
    "assert isinstance(stu_ans[1], float), \"Q4: tuple[1] should be a float. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11ae3ab99ef353e74176d0a0d297b340",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5: Train an $n$-gram language model (10 pts)\n",
    "\n",
    "Now we are well positioned to start training an $n$-gram language model. We can fit a language model using the `MLE` class from `nltk.lm`. It requires two inputs: a list of all $n$-grams for each sentence and a vocabulary, both of which you have already written a function to build. Now it's time to put them together to work. \n",
    "\n",
    "**Complete the function below to return an `nltk.lm.MLE` object representing a trained $n$-gram language model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c62ee69084d283bba34ee6a78a90907",
     "grade": false,
     "grade_id": "cell-6136d38621377eeb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "def train_ngram_lm(n):\n",
    "    \"\"\"\n",
    "    Train a n-gram language model as specified by the argument \"n\"\n",
    "    \"\"\"\n",
    "\n",
    "    lm = MLE(n)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    sentences = load_data()\n",
    "    vocab = build_vocab(sentences)\n",
    "    \n",
    "    for i in range(n, 0, -1):\n",
    "        lm.fit(build_ngrams(i, sentences), vocab)\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9255397707efc8ad990ed00ec213c5e",
     "grade": true,
     "grade_id": "cell-9c44953c467910db",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_n = 4\n",
    "stu_lm = train_ngram_lm(stu_n)\n",
    "stu_vocab = build_vocab(load_data())\n",
    "\n",
    "assert isinstance(stu_lm, nltk.lm.MLE), \"Q3b: Your function should return an nltk.lm.MLE object. \"\n",
    "\n",
    "assert hasattr(stu_lm, \"vocab\") and len(stu_lm.vocab) == len(stu_vocab) + 1, \"Q3b: Your language model wasn't trained properly. \"\n",
    "\n",
    "del stu_n, stu_lm, stu_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a096876626b3ba060807018d89d3efd2",
     "grade": false,
     "grade_id": "cell-2fc6d275a5179f63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "FINALLY, are you ready to compose sonnets like the real Shakespeare?! We provide some starter code below, but absolutely feel free to modify any parts of it on your own. It'd be interesting to see how the \"authenticity\" of the sonnets is related to the parameter $n$. Do the sonnets feel more Shakespeare when you increase $n$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but if thou live remembered not to be\n",
      "making dead wood more blest than living lips\n",
      "beggared of blood to blush through lively veins\n",
      "then bettered that the world may see my\n",
      "and right perfection wrongfully disgraced ,\n",
      "and there reigns love and all love ’\n",
      "or some fierce thing replete with too much\n",
      "shall hate be fairer lodged than gentle love\n",
      "whilst my poor lips which should that harvest\n",
      "to know my shames and praises from your\n",
      "than that , which on thy humour doth\n",
      "and your true rights be termed a poet\n",
      "what need ’ st thou wound with cunning\n",
      "i summon up remembrance of things past ,\n"
     ]
    }
   ],
   "source": [
    "# Every time it runs, depending on how drunk it is, a different sonnet is written. \n",
    "n = 6\n",
    "num_lines = 14\n",
    "num_words_per_line = 8\n",
    "text_seed = [\"<s>\"] * (n - 1)\n",
    "\n",
    "lm = train_ngram_lm(n)\n",
    "\n",
    "sonnet = []\n",
    "while len(sonnet) < num_lines:\n",
    "    while True:  # keep generating a line until success\n",
    "        try:\n",
    "            line = lm.generate(num_words_per_line, text_seed=text_seed)\n",
    "        except ValueError:  # the generation is not always successful. need to capture exceptions\n",
    "            continue\n",
    "        else:\n",
    "            line = [x for x in line if x not in [\"<s>\", \"</s>\"]]\n",
    "            sonnet.append(\" \".join(line))\n",
    "            break\n",
    "\n",
    "# pretty-print your sonnet\n",
    "print(\"\\n\".join(sonnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_ii_v1_assignment1_part2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
