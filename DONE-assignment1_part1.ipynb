{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "288260708ed53d23fbee2b1333a77e8a",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1 Part 1: N-gram Language Models (40 pts)\n",
    "\n",
    "In this assignment, we're going to train an n-gram language model that is able to \"imitate\" William Shakespeare's writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d11c9af18458f110af7afffe909f9b86",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Load the dataset (10 pts)\n",
    "\n",
    "As the first step towards imitating Shakespeare's writing, you will create a function called `load_data` that loads his original *Sonnets* from `assets/gutenberg/THE_SONNETS.txt`. This function should accomplish the following: \n",
    "\n",
    "* **Extract sentences from the data file.** Of course, depending on the nature of the task at hand, what constitutes a *sentence* can vary. In the context of this assignment, we will define a sentence as just a line of a sonnet, regardless of the punctuation at end. In addition, we will ignore the boundaries of the sonnets --- that is, we are not dealing with $154$ individual *sonnets* but rather $154 \\times 14 = 2156$ *sentences* (actually only $2155$ sentences, as *Sonnet 99* has [15 lines](https://www.google.com/search?ei=po4hX9jzN4rKtQaL7K-wDg&q=why+is+sonnet+99+15+lines&oq=why+is+sonnet+99+15+lines&gs_lcp=CgZwc3ktYWIQAzoECAAQR1ChGlihGmCqHGgAcAJ4AIABXIgBXJIBATGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjY3snX3PLqAhUKZc0KHQv2C-YQ4dUDCAw&uact=5) but *Sonnet 126* has only [12](https://www.google.com/search?ei=d4whX6_uH9a4tQbUiISoDA&q=why+is+sonnet+126+only+12+lines&oq=o+thou+my+lovely+boy+who+in+thy+power&gs_lcp=CgZwc3ktYWIQARgBMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHUABYAGCWEWgAcAJ4AIABAIgBAJIBAJgBAKoBB2d3cy13aXrAAQE&sclient=psy-ab)). We encourage you to explore alternative definitions of a sentence on your own; for example, an entire sonnet could be modelled as a sentence. Finally, make sure that the newline character `\\n` at the end of each line is dropped. \n",
    "\n",
    "\n",
    "* **Tokenise each extracted sentence.** While it's ambiguous what a sentence is, what constitutes a \"*word*\" is even more task-dependent. Do punctuations count as \"words\"? Are two \"words\" with the same spelling but different casing considered identical? Since what a text file contains is nothing more than a squence of characters, there are many possible ways of grouping these characters to form \"words\" that are subsequently taken as input by a program. To distinguish what's actually taken as input by a program from a linguistic word, we call the former a *token*. The process of producing a list of tokens out of a sentence is then called *tokenisation*. At this step, you will first lower-case each sentence extracted from the previous step entirely and then tokenise each lower-cased sentence. You may use any tokeniser of your choice, such as `word_tokenize` from the `nltk` library. The grading of the assignment doesn't depend on your choice of the tokeniser. \n",
    "\n",
    "**This function should return a `list` of length 2155, where each element is a `list` of `str` representing the tokens of each sentence as produced by the tokeniser of your choice. An example output would be:**\n",
    "\n",
    "```\n",
    "[['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ','],\n",
    " ['that', 'thereby', 'beauty', '’', 's', 'rose', 'might', 'never', 'die', ','],\n",
    " ...\n",
    " ['came', 'there', 'for', 'cure', 'and', 'this', 'by', 'that', 'i', 'prove', ','], \n",
    " ['love', '’', 's', 'fire', 'heats', 'water', ',', 'water', 'cools', 'not', 'love', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d4ea2201f33d2455eb83f6c7c68e483",
     "grade": false,
     "grade_id": "cell-876ee5c9fdc4f347",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize, TweetTokenizer, word_tokenize\n",
    "    import re\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    with open('assets/gutenberg/THE_SONNETS.txt', 'r') as file:\n",
    "        sentences = file.read()[3:]\n",
    "        \n",
    "    # Fix decoding error\n",
    "    sentences = sentences.encode('cp1252') \n",
    "    sentences = sentences.decode('utf-8')\n",
    "    \n",
    "    sentences = [i.strip(' ').lower() for i in sentences.split('\\n') if len(i.strip(' ').split(' ')) > 3]\n",
    "    sentences = [nltk.word_tokenize(i) for i in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "stu_ans = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2155"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stu_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ','],\n",
       " ['that', 'thereby', 'beauty', '’', 's', 'rose', 'might', 'never', 'die', ','],\n",
       " ['but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ','],\n",
       " ['his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':'],\n",
       " ['but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ',']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e4184bbeb9f594851ab27528c1c385e",
     "grade": true,
     "grade_id": "cell-436f8cfe2405ffe8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test\n",
    "\n",
    "stu_ans = load_data()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q1: Your function should return a list. \"\n",
    "assert len(stu_ans) == 2155, \"Q1: There should be 2155 sentences. \"\n",
    "\n",
    "for index, tokens in enumerate(stu_ans):\n",
    "    assert isinstance(tokens, list), f\"Q1: The element at index {index} of your answer list should be a list. \"\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            assert token.islower(), f'Q1: Token \"{token}\" in the sentence at index {index} is not lower-cased. '\n",
    "        \n",
    "        assert token != \"\\n\", f'Q1: You should drop the \"\\\\n\" character in the sentence at index {index}. '\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62ca73819ccfb33c161ca8f1298de083",
     "grade": false,
     "grade_id": "cell-902e515e7633ecbd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Build vocabulary (15 pts)\n",
    "\n",
    "Next, we need a \"vocabulary\" that contains all the unique tokens. Moreover, as mentioned in the lecture, we often pad a sentence with `<s>` and `</s>` to indicate its start and end when working with n-gram language models; therefore, these two special tokens should also be included in our vocabulary. Complete the function below to build a vocabulary. The order in which the tokens are stored doesn't matter. \n",
    "\n",
    "**This function should return a `list` of unique tokens, including `<s>` and `</s>`. An example output would be:**\n",
    "\n",
    "```\n",
    "['refuse', 'enjoyed', ..., '<s>', '</s>']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(str_list): # helper function for the build_vocab function\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for x in str_list:\n",
    "        if x not in output:\n",
    "            output.append(x)\n",
    "    output.append('<s>')\n",
    "    output.append('</s>')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19516439848342b7eedc50ef78c20fc7",
     "grade": false,
     "grade_id": "cell-be2a75b95ae4c066",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = []\n",
    "    sentences_flat = [item for sublist in sentences for item in sublist]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    vocab = unique(sentences_flat)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "stu_sents = load_data()\n",
    "stu_vocab = build_vocab(stu_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b9d2caf50e94b70e0c65ad9b18e3818",
     "grade": true,
     "grade_id": "cell-bd592f77659516e2",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_sents = load_data()\n",
    "stu_vocab = build_vocab(stu_sents)\n",
    "\n",
    "assert isinstance(stu_vocab, list), \"Q2: Your function should return a list. \"\n",
    "assert stu_vocab, \"Q2: Your vocab is empty. \"\n",
    "assert \"<s>\" in stu_vocab, \"Q2: Remember to include special token <s>. \"\n",
    "assert \"</s>\" in stu_vocab, \"Q2: Remember to include special token </s>. \"\n",
    "assert len(set(stu_vocab)) == len(stu_vocab), \"Q2: Your vocab contains duplicated tokens. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_sents, stu_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7c6f4ee7affc1ab88ddd68db23652d7",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3: Generate all $n$-grams (15 pts)\n",
    "\n",
    "Now let's write a function to generate all $n$-grams for each sentence. This can be accomplished in two steps:\n",
    "* **Pad each sentence with `<s>` and `</s>` for $n \\geq 2$**. You need $n - 1$ paddings on both ends of a sentence, so that there are two $n$-grams that model the first and the last token, respectively. You may implement the padding function yourself or use the `pad_both_ends` function from the `nltk` library. \n",
    "\n",
    "\n",
    "* **Generate $n$-grams on the padded sentences**. Check out the `ngrams` function from `nltk`. For a sentence with $\\ell$ tokens excluding paddings, the maximum number of possible $n$-grams generated from its padded version should be $\\ell + n - 1$. Think about why. \n",
    "\n",
    "**Complete the function below to return a `list`, where each element of the `list` is a either a list or a \"generator object\" produced by the `ngrams` function, representing a sequence of all $n$-grams generated from each appropriately padded sentence. If the argument `n=2`, the autograder would accept either of the example outputs below:**\n",
    "\n",
    "```\n",
    "[<generator object ngrams at 0x7f77ed778b10>,\n",
    " <generator object ngrams at 0x7f77e7d934f8>,\n",
    " ...\n",
    " <generator object ngrams at 0x7f77e7d751b0>,\n",
    " <generator object ngrams at 0x7f77e7d75228>]\n",
    "```\n",
    "\n",
    "**OR**\n",
    "\n",
    "```\n",
    "[\n",
    "    [('<s>', 'from'), ('from', 'fairest'), ('fairest', 'creatures'), ('creatures', 'we'), ('we', 'desire'),\n",
    "     ('desire', 'increase'), ('increase', ','), (',', '</s>')], \n",
    "    \n",
    "    [('<s>', 'that'), ('that', 'thereby'), ('thereby', 'beauty'), ('beauty', '’'), ('’', 's'), ('s', 'rose'), \n",
    "     ('rose', 'might'), ('might', 'never'), ('never', 'die'), ('die', ','), (',', '</s>')],\n",
    "     \n",
    "    ...\n",
    "    \n",
    "    [('<s>', 'came'), ('came', 'there'), ('there', 'for'), ('for', 'cure'), ('cure', 'and'), ('and', 'this'), \n",
    "     ('this', 'by'), ('by', 'that'), ('that', 'i'), ('i', 'prove'), ('prove', ','), (',', '</s>')], \n",
    "    \n",
    "    [('<s>', 'love'), ('love', '’'), ('’', 's'), ('s', 'fire'), ('fire', 'heats'), ('heats', 'water'), \n",
    "     ('water', ','), (',', 'water'), ('water', 'cools'), ('cools', 'not'), ('not', 'love'), ('love', '.'), \n",
    "     ('.', '</s>')]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c82c4eeada18079f4459f3e5d7242912",
     "grade": false,
     "grade_id": "cell-79211162e032488b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for sample in sentences:\n",
    "        \n",
    "        tmp = sample.copy()\n",
    "        \n",
    "        for i in range(n-1):\n",
    "            tmp.insert(0, '<s>')\n",
    "            tmp.append('</s>')\n",
    "            \n",
    "        all_ngrams.append([i for i in ngrams(tmp, n)])\n",
    "\n",
    "    return all_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39aa98a54d9d0e6bee01e17de33dfe80",
     "grade": true,
     "grade_id": "cell-66bf33f44f8b83bf",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_n = 4\n",
    "stu_sents = load_data()\n",
    "old_hash = hash(tuple([tuple(sent) for sent in stu_sents]))\n",
    "stu_ngrams = build_ngrams(stu_n, stu_sents)\n",
    "\n",
    "assert isinstance(stu_ngrams, list), \"Q3: Your function should return a list. \"\n",
    "assert stu_ngrams, \"Q3: Your ngrams list is empty. \"\n",
    "\n",
    "# Check that your function does not modify 'stu_sents' in place\n",
    "new_hash = hash(tuple([tuple(sent) for sent in stu_sents]))\n",
    "assert new_hash == old_hash, \"Q3: Your function should not modify its argument 'sentences' in place. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del stu_n, stu_sents, stu_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad086d5cfb862a0e623e123e8fe7e500",
     "grade": false,
     "grade_id": "cell-caf5a40a85230775",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have completed all the preparation work for imitating William Shakespeare's writing, it's time to take a break. We will resume in Assignment 1 Part 2 to finish training an $n$-gram language model. See you there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_ii_v1_assignment1_part1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
